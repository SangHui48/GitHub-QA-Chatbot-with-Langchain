import streamlit as st
from dotenv import load_dotenv
from streamlit_extras.add_vertical_space import add_vertical_space
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from githubqa.data_processing import dictionary_to_docs
from githubqa.get_info_from_api import github_api_call
from githubqa.vector_db import (
    db_from_pinecone, db_from_deeplake, mmr_retriever_setting
)

# Sidebar contents
with st.sidebar:
    st.set_page_config(page_title = "This is a Multipage WebApp")
    st.title('ğŸ¤—ğŸ’¬ LLM Chat App')
    add_vertical_space(5)
    st.write('Made with  by [ì˜¤ë¯¸ìì°¨](https://github.com/SangHui48/KDT_AI_B3)')
    

def main():
    load_dotenv()
    MODEL_NAME = "gpt-3.5-turbo-16k" # langchain llm config

    st.header("Gitter:feather: ")

    # user input github repo url
    github_link = st.text_input("Github repository linkì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    if github_link:
        with st.spinner('ë ˆí¬ì§€í„°ë¦¬ ë¶„ì„ì¤‘...'):
            # 2. ëª¨ë“  ë°ì´í„° "File_name" : "File_content" í˜•ì‹ ë°›ì•„ì˜¤ê¸°
            github_info_dict, structure_content = github_api_call(github_link)

        # 3. "File_content í˜•ì‹ ë°ì´í„°" ì²­í‚¹ ê°¯ìˆ˜ ë‹¨ìœ„ë¡œ ìë¥¸í›„ì— ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸°
        # ë°˜í™˜ê°’ [Doc1, Doc2 ...]
        with st.spinner('ì„ë² ë”©ì¤‘...'):
            docs = dictionary_to_docs(
                github_info_dict, structure_content,
                chunking_size=1000, overlap_size=0, 
                model_name=MODEL_NAME
            )
            # 4. chunking ëœ ë°ì´í„° vector db ë¡œ ì„ë² ë”© í•˜ê¸° 
            # ì„ë² ë”© ëª¨ë¸ ë° vector db ë°˜í™˜ 
            embedding_model = OpenAIEmbeddings(model='text-embedding-ada-002')
            # vector_db = db_from_deeplake(docs, embedding_model)
            vector_db = db_from_pinecone(docs, embedding_model)
            
            # 5. QA ë¥¼ ìœ„í•œ retriever ë° qa ì„¸íŒ… í•˜ê¸°
            retriever =  mmr_retriever_setting(
                vectorstore=vector_db, 
                fetch_num=10, k_num=100
            )
        open_ai_model =  ChatOpenAI(model_name=MODEL_NAME)
        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        qa_chain = ConversationalRetrievalChain.from_llm(
            llm=open_ai_model,
            memory=memory,
            retriever=retriever,
            get_chat_history=lambda h : h,
        )
        #QA ì‹œì‘
        query = st.chat_input("Your message: ", key="user_input")
        if query:
            st.session_state.messages.append(query)
            with st.spinner("ë‹µë³€ ìƒì„±ì¤‘..."):
                response = qa_chain({"question": query})
                st.session_state.messages.append(response["answer"])
                    
            messages = st.session_state.get('messages', [])
            for i, msg in enumerate(messages):
                if i % 2 == 0:
                    with st.chat_message("user"):
                        st.write(msg, key=str(i) + '_user')
                else:
                    with st.chat_message("assistant"):
                        st.write(msg, key=str(i) + '_ai')

if __name__ == '__main__':
    main()